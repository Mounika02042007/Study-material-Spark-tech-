# -*- coding: utf-8 -*-
"""Study(Spark Tech).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PMTv-RwsAxeputxWgYdDNoazx8jOGPOh
"""



!pip install -q transformers torch accelerate pypdf2 sentence-transformers faiss-cpu gradio huggingface_hub





# StudyMate: AI-Powered PDF Q&A System
# Complete implementation for Google Colab

# Install required packages
!pip install -q transformers torch accelerate pymupdf sentence-transformers faiss-cpu gradio huggingface_hub

import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import fitz  # PyMuPDF
import gradio as gr
from typing import List, Tuple
import warnings
warnings.filterwarnings('ignore')

print("üìö Initializing StudyMate...")

# Initialize models
print("Loading IBM Granite 3.1 2B Instruct model...")
model_name = "ibm-granite/granite-3.1-2b-instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto",
    low_cpu_mem_usage=True
)

print("Loading embedding model for semantic search...")
embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

# Global variables for document processing
pdf_chunks = []
chunk_embeddings = None
faiss_index = None
pdf_text_cache = ""

def extract_text_from_pdf(pdf_path: str) -> str:
    """Extract text content from uploaded PDF file using PyMuPDF"""
    try:
        # Open PDF with PyMuPDF
        doc = fitz.open(pdf_path)
        text = ""

        print(f"Processing {len(doc)} pages...")

        for page_num in range(len(doc)):
            page = doc[page_num]
            # Extract text from page
            page_text = page.get_text()
            text += page_text + "\n\n"

        doc.close()

        # Clean up text
        text = text.strip()

        if not text or len(text) < 50:
            return "Error: PDF appears to be empty or contains no extractable text. It might be a scanned document or image-based PDF."

        print(f"Extracted {len(text)} characters from PDF")
        return text

    except Exception as e:
        return f"Error extracting PDF: {str(e)}"

def chunk_text(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:
    """Split text into overlapping chunks for better context retrieval"""
    words = text.split()
    chunks = []

    if len(words) == 0:
        return chunks

    for i in range(0, len(words), chunk_size - overlap):
        chunk = " ".join(words[i:i + chunk_size])
        if chunk.strip() and len(chunk.strip()) > 20:  # Only add meaningful chunks
            chunks.append(chunk.strip())

    return chunks

def create_faiss_index(chunks: List[str]) -> Tuple[np.ndarray, faiss.Index]:
    """Create FAISS index for semantic search"""
    print(f"Creating embeddings for {len(chunks)} chunks...")
    embeddings = embedding_model.encode(chunks, show_progress_bar=True)
    embeddings = np.array(embeddings).astype('float32')

    # Normalize embeddings for cosine similarity
    faiss.normalize_L2(embeddings)

    # Create FAISS index
    dimension = embeddings.shape[1]
    index = faiss.IndexFlatIP(dimension)  # Inner product for cosine similarity
    index.add(embeddings)

    print(f"FAISS index created with {index.ntotal} vectors")
    return embeddings, index

def retrieve_relevant_chunks(query: str, top_k: int = 3) -> List[str]:
    """Retrieve most relevant chunks using semantic search"""
    global pdf_chunks, faiss_index

    if faiss_index is None or len(pdf_chunks) == 0:
        return []

    # Encode query
    query_embedding = embedding_model.encode([query])
    query_embedding = np.array(query_embedding).astype('float32')
    faiss.normalize_L2(query_embedding)

    # Search in FAISS index
    top_k = min(top_k, len(pdf_chunks))  # Don't ask for more chunks than available
    distances, indices = faiss_index.search(query_embedding, top_k)

    # Return relevant chunks
    relevant_chunks = [pdf_chunks[idx] for idx in indices[0] if idx < len(pdf_chunks)]
    return relevant_chunks

def generate_answer(query: str, context: str) -> str:
    """Generate answer using IBM Granite model"""
    prompt = f"""You are StudyMate, an AI academic assistant. Based on the provided context from study materials, answer the student's question accurately and comprehensively.

Context from study materials:
{context}

Student's Question: {query}

Answer (provide a detailed, well-structured response):"""

    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048)
    inputs = {k: v.to(model.device) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=512,
            temperature=0.7,
            top_p=0.9,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )

    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # Extract only the answer part
    if "Answer (provide a detailed, well-structured response):" in answer:
        answer = answer.split("Answer (provide a detailed, well-structured response):")[-1].strip()

    return answer

def process_pdf(pdf_file):
    """Process uploaded PDF and create searchable index"""
    global pdf_chunks, chunk_embeddings, faiss_index, pdf_text_cache

    if pdf_file is None:
        return "‚ö†Ô∏è Please upload a PDF file first."

    try:
        # Extract text
        print("Extracting text from PDF...")
        text = extract_text_from_pdf(pdf_file)

        if text.startswith("Error"):
            return text

        pdf_text_cache = text

        # Chunk text
        print("Chunking text...")
        pdf_chunks = chunk_text(text)

        if len(pdf_chunks) == 0:
            return "‚ö†Ô∏è No meaningful text could be extracted from the PDF. The file might be:\n- A scanned document (image-based)\n- Password protected\n- Corrupted\n\nTry using a text-based PDF or OCR the document first."

        # Create FAISS index
        print("Creating semantic search index...")
        chunk_embeddings, faiss_index = create_faiss_index(pdf_chunks)

        return f"‚úÖ PDF processed successfully!\n\nüìä Statistics:\n- Total text length: {len(text):,} characters\n- Number of searchable chunks: {len(pdf_chunks)}\n- Ready to answer questions!"

    except Exception as e:
        return f"‚ùå Error processing PDF: {str(e)}\n\nPlease make sure:\n- The PDF is not corrupted\n- The PDF contains text (not just images)\n- The file is a valid PDF format"

def answer_question(question: str, pdf_file):
    """Main function to answer questions based on PDF content"""
    global pdf_chunks, faiss_index

    if pdf_file is None:
        return "‚ö†Ô∏è Please upload a PDF file first."

    if faiss_index is None or len(pdf_chunks) == 0:
        return "‚ö†Ô∏è Please click 'Process PDF' button first to analyze your document."

    if not question.strip():
        return "‚ö†Ô∏è Please enter a question."

    try:
        # Retrieve relevant context
        print(f"Searching for relevant content for: {question}")
        relevant_chunks = retrieve_relevant_chunks(question, top_k=3)

        if not relevant_chunks:
            return "‚ö†Ô∏è Could not find relevant information in the PDF."

        context = "\n\n".join(relevant_chunks)

        # Generate answer
        print("Generating answer...")
        answer = generate_answer(question, context)

        return f"üìñ **Answer:**\n\n{answer}\n\n---\n\nüí° **Sources:** Answer generated from {len(relevant_chunks)} relevant sections of your study material."

    except Exception as e:
        return f"‚ùå Error generating answer: {str(e)}"

def clear_data():
    """Clear all processed data"""
    global pdf_chunks, chunk_embeddings, faiss_index, pdf_text_cache
    pdf_chunks = []
    chunk_embeddings = None
    faiss_index = None
    pdf_text_cache = ""
    return "üóëÔ∏è All data cleared. You can upload a new PDF."

def view_extracted_text():
    """Show a preview of extracted text"""
    global pdf_text_cache
    if not pdf_text_cache:
        return "‚ö†Ô∏è No PDF processed yet. Please upload and process a PDF first."

    preview = pdf_text_cache[:1000]  # Show first 1000 characters
    return f"üìÑ **Extracted Text Preview** (first 1000 characters):\n\n{preview}\n\n... (Total: {len(pdf_text_cache):,} characters)"

def generate_summary():
    """Generate a comprehensive summary of the PDF content"""
    global pdf_text_cache

    if not pdf_text_cache:
        return "‚ö†Ô∏è No PDF processed yet. Please upload and process a PDF first."

    try:
        # Use first 3000 characters for summary to stay within context limits
        content = pdf_text_cache[:3000]

        prompt = f"""You are StudyMate, an AI academic assistant. Provide a comprehensive summary of the following study material. Focus on the main topics, key concepts, and important information.

Study Material:
{content}

Summary:"""

        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048)
        inputs = {k: v.to(model.device) for k, v in inputs.items()}

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=400,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )

        summary = tokenizer.decode(outputs[0], skip_special_tokens=True)

        if "Summary:" in summary:
            summary = summary.split("Summary:")[-1].strip()

        return f"üìù **Document Summary:**\n\n{summary}"

    except Exception as e:
        return f"‚ùå Error generating summary: {str(e)}"

def extract_key_points():
    """Extract key points and important concepts from the PDF"""
    global pdf_text_cache

    if not pdf_text_cache:
        return "‚ö†Ô∏è No PDF processed yet. Please upload and process a PDF first."

    try:
        content = pdf_text_cache[:3000]

        prompt = f"""You are StudyMate, an AI academic assistant. Extract the key points, main concepts, and important information from the following study material. Present them as a clear, organized list.

Study Material:
{content}

Key Points:"""

        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048)
        inputs = {k: v.to(model.device) for k, v in inputs.items()}

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=400,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )

        key_points = tokenizer.decode(outputs[0], skip_special_tokens=True)

        if "Key Points:" in key_points:
            key_points = key_points.split("Key Points:")[-1].strip()

        return f"üéØ **Key Points & Main Concepts:**\n\n{key_points}"

    except Exception as e:
        return f"‚ùå Error extracting key points: {str(e)}"

def generate_quiz(num_questions: int = 5):
    """Generate quiz questions based on the PDF content"""
    global pdf_text_cache

    if not pdf_text_cache:
        return "‚ö†Ô∏è No PDF processed yet. Please upload and process a PDF first."

    try:
        content = pdf_text_cache[:3000]

        prompt = f"""You are StudyMate, an AI academic assistant. Create {num_questions} multiple-choice quiz questions based on the following study material. For each question, provide 4 options (A, B, C, D) and indicate the correct answer.

Study Material:
{content}

Quiz Questions:"""

        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048)
        inputs = {k: v.to(model.device) for k, v in inputs.items()}

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=500,
                temperature=0.8,
                top_p=0.9,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )

        quiz = tokenizer.decode(outputs[0], skip_special_tokens=True)

        if "Quiz Questions:" in quiz:
            quiz = quiz.split("Quiz Questions:")[-1].strip()

        return f"üìù **Practice Quiz:**\n\n{quiz}\n\n---\nüí° Use this quiz to test your understanding of the material!"

    except Exception as e:
        return f"‚ùå Error generating quiz: {str(e)}"

def create_study_notes():
    """Create structured study notes from the PDF"""
    global pdf_text_cache

    if not pdf_text_cache:
        return "‚ö†Ô∏è No PDF processed yet. Please upload and process a PDF first."

    try:
        content = pdf_text_cache[:3000]

        prompt = f"""You are StudyMate, an AI academic assistant. Create organized study notes from the following study material. Structure the notes with clear sections, important definitions, key concepts, and examples where applicable.

Study Material:
{content}

Study Notes:"""

        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048)
        inputs = {k: v.to(model.device) for k, v in inputs.items()}

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=450,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )

        notes = tokenizer.decode(outputs[0], skip_special_tokens=True)

        if "Study Notes:" in notes:
            notes = notes.split("Study Notes:")[-1].strip()

        return f"üìö **Study Notes:**\n\n{notes}"

    except Exception as e:
        return f"‚ùå Error creating study notes: {str(e)}"

# Create Gradio Interface
with gr.Blocks(theme=gr.themes.Soft(), title="StudyMate - AI PDF Q&A") as demo:
    gr.Markdown("""
    # üìö StudyMate: AI-Powered PDF-Based Q&A System

    **An intelligent academic assistant powered by IBM Granite 3.1 2B Instruct**

    Upload your study materials (textbooks, lecture notes, research papers) and access powerful AI features!
    """)

    with gr.Row():
        with gr.Column(scale=1):
            pdf_input = gr.File(
                label="üìÑ Upload PDF Study Material",
                file_types=[".pdf"],
                type="filepath"
            )
            process_btn = gr.Button("üîÑ Process PDF", variant="primary", size="lg")

            gr.Markdown("### üìä Document Management")
            preview_btn = gr.Button("üëÅÔ∏è Preview Extracted Text")
            clear_btn = gr.Button("üóëÔ∏è Clear Data")

            status_output = gr.Textbox(
                label="üìä Status",
                lines=6,
                interactive=False
            )

        with gr.Column(scale=2):
            with gr.Tabs():
                with gr.Tab("‚ùì Q&A"):
                    gr.Markdown("### Ask Questions About Your Study Material")
                    question_input = gr.Textbox(
                        label="Your Question",
                        placeholder="e.g., What are the main concepts discussed in Chapter 3?",
                        lines=2
                    )
                    submit_btn = gr.Button("üöÄ Get Answer", variant="primary", size="lg")
                    answer_output = gr.Textbox(
                        label="Answer",
                        lines=12,
                        interactive=False
                    )

                with gr.Tab("üìù Summary"):
                    gr.Markdown("### Get a Comprehensive Summary of Your Document")
                    summary_btn = gr.Button("üìù Generate Summary", variant="primary", size="lg")
                    summary_output = gr.Textbox(
                        label="Document Summary",
                        lines=12,
                        interactive=False
                    )

                with gr.Tab("üéØ Key Points"):
                    gr.Markdown("### Extract Key Points and Main Concepts")
                    keypoints_btn = gr.Button("üéØ Extract Key Points", variant="primary", size="lg")
                    keypoints_output = gr.Textbox(
                        label="Key Points",
                        lines=12,
                        interactive=False
                    )

                with gr.Tab("üìù Quiz Generator"):
                    gr.Markdown("### Generate Practice Questions to Test Your Knowledge")
                    num_questions = gr.Slider(
                        minimum=3,
                        maximum=10,
                        value=5,
                        step=1,
                        label="Number of Questions"
                    )
                    quiz_btn = gr.Button("üìù Generate Quiz", variant="primary", size="lg")
                    quiz_output = gr.Textbox(
                        label="Practice Quiz",
                        lines=12,
                        interactive=False
                    )

                with gr.Tab("üìö Study Notes"):
                    gr.Markdown("### Create Organized Study Notes from Your Material")
                    notes_btn = gr.Button("üìö Create Study Notes", variant="primary", size="lg")
                    notes_output = gr.Textbox(
                        label="Study Notes",
                        lines=12,
                        interactive=False
                    )

    gr.Markdown("""
    ---
    ### üéØ Features:

    1. **Q&A Mode**: Ask natural language questions and get contextual answers from your study material
    2. **Smart Summary**: Get a comprehensive overview of your document's main topics and key information
    3. **Key Points Extraction**: Automatically identify and list the most important concepts and information
    4. **Quiz Generator**: Create practice questions with multiple choice options to test your understanding
    5. **Study Notes**: Generate well-structured, organized notes perfect for exam preparation

    ### üîç Technology Stack:
    - **Semantic Search**: FAISS + Sentence Transformers for accurate content retrieval
    - **Text Extraction**: PyMuPDF for high-quality PDF processing
    - **AI Generation**: IBM Granite 3.1 2B Instruct for intelligent responses

    ### üí° Tips:
    - Upload text-based PDF files (not scanned images)
    - Click "Process PDF" after uploading to analyze the document
    - Use "Preview Extracted Text" to verify text extraction worked
    - Try all features to get comprehensive study support!

    ### ‚ö†Ô∏è Note:
    - Features work best with text-based PDFs
    - Processing time depends on document size
    - For scanned/image PDFs, consider OCR preprocessing
    """)

    # Event handlers - Document Management
    process_btn.click(
        fn=process_pdf,
        inputs=[pdf_input],
        outputs=[status_output]
    )

    preview_btn.click(
        fn=view_extracted_text,
        inputs=[],
        outputs=[status_output]
    )

    clear_btn.click(
        fn=clear_data,
        inputs=[],
        outputs=[status_output]
    )

    # Event handlers - Features
    submit_btn.click(
        fn=answer_question,
        inputs=[question_input, pdf_input],
        outputs=[answer_output]
    )

    summary_btn.click(
        fn=generate_summary,
        inputs=[],
        outputs=[summary_output]
    )

    keypoints_btn.click(
        fn=extract_key_points,
        inputs=[],
        outputs=[keypoints_output]
    )

    quiz_btn.click(
        fn=generate_quiz,
        inputs=[num_questions],
        outputs=[quiz_output]
    )

    notes_btn.click(
        fn=create_study_notes,
        inputs=[],
        outputs=[notes_output]
    )

# Launch the interface
print("\n‚úÖ StudyMate is ready!")
print("üöÄ Launching Gradio interface...\n")

demo.launch(debug=True, share=True)



